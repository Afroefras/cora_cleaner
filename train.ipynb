{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pydub import AudioSegment\n",
    "from scripts.plot import plot_audio_sample\n",
    "from scripts.extract import load_heart_noised_paths\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_noised = load_heart_noised_paths(\n",
    "    clean_dir=\"data/heart_sound_test_small\",\n",
    "    noised_dir=\"data/heart_noised_test_small\",\n",
    ")\n",
    "\n",
    "heart_noised[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo suena?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = heart_noised[22]\n",
    "\n",
    "audio_clean = AudioSegment.from_file(test[0])\n",
    "audio_noisy = AudioSegment.from_file(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo se ve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_array = np.array(audio_clean.get_array_of_samples())\n",
    "noisy_array = np.array(audio_noisy.get_array_of_samples())\n",
    "\n",
    "plot_audio_sample(clean_array, 'Audio Limpio')\n",
    "plot_audio_sample(noisy_array, 'Audio con Ruido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_array.shape, noisy_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        clean, noisy = batch\n",
    "        # Forward pass\n",
    "        decoded = self(noisy)\n",
    "        # Calculamos la pérdida (error de reconstrucción)\n",
    "        train_loss = nn.MSELoss()(decoded, clean)\n",
    "        # Registramos la pérdida para su monitoreo\n",
    "        self.log('train_loss', train_loss, on_epoch=True)\n",
    "\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        clean, noisy = batch\n",
    "        decoded = self(noisy)\n",
    "        val_loss = nn.MSELoss()(decoded, clean)\n",
    "        self.log('val_loss', val_loss, on_epoch=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        clean, noisy = batch\n",
    "        decoded = self(noisy)\n",
    "        test_loss = nn.MSELoss()(decoded, clean)\n",
    "        self.log('test_loss', test_loss)\n",
    "\n",
    "        return {\n",
    "            'test_loss': test_loss,\n",
    "            'reconstructed': decoded,\n",
    "            'clean': clean\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        # optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, cycle_momentum=False),\n",
    "            'interval': 'step',     # Frecuencia de ajuste del LR scheduler (en cada paso)\n",
    "        }\n",
    "\n",
    "        # scheduler = {\n",
    "        #     'scheduler': ReduceLROnPlateau(optimizer, patience=3),\n",
    "        #     'monitor': 'val_loss',  # Métrica para monitorear\n",
    "        #     'interval': 'epoch',    # Frecuencia de ajuste del LR scheduler\n",
    "        #     'frequency': 1          # Igual a interval, ya que estamos usando 'epoch'\n",
    "        # }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoraCleanerDataset(Dataset):\n",
    "    def __init__(self, data_tuples, transform=None):\n",
    "        self.data_tuples = data_tuples\n",
    "        self.transform = transform\n",
    "        # Calcular la duración mínima de todos los audios\n",
    "        self.min_duration = self.calculate_min_duration()\n",
    "\n",
    "    def calculate_min_duration(self):\n",
    "        self.durations = []\n",
    "        for _, audio_path in self.data_tuples:\n",
    "            audio, _ = torchaudio.load(audio_path)\n",
    "            self.durations.append(audio.shape[-1])\n",
    "\n",
    "        return min(self.durations)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tuples)\n",
    "    \n",
    "    def adjust_audio_duration(self, audio, target_duration):\n",
    "        \"\"\"\n",
    "        Ajusta la duración de un audio a la duración objetivo.\n",
    "\n",
    "        Args:\n",
    "            audio (torch.Tensor): Audio a ajustar.\n",
    "            target_duration (int): Duración objetivo.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Audio ajustado.\n",
    "        \"\"\"\n",
    "        original_duration = audio.shape[-1]\n",
    "        \n",
    "        if original_duration <= target_duration:\n",
    "            return audio\n",
    "        \n",
    "        # Truncar o recortar el audio al mínimo de las duraciones\n",
    "        adjusted_audio = audio[..., :target_duration]\n",
    "\n",
    "        return adjusted_audio\n",
    "    \n",
    "    def normalize_audio(self, audio):\n",
    "        mean = audio.mean()\n",
    "        std = audio.std()\n",
    "        normalized = (audio - mean) / std\n",
    "        normalized.unsqueeze_(dim=0)\n",
    "        return normalized.reshape(1, 1, -1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_audio_path = self.data_tuples[idx][0]\n",
    "        clean_audio, sample_rate = torchaudio.load(clean_audio_path)\n",
    "\n",
    "        clean_audio = self.normalize_audio(clean_audio)\n",
    "        clean_audio = self.adjust_audio_duration(clean_audio, self.min_duration)\n",
    "\n",
    "\n",
    "        noisy_audio_path = self.data_tuples[idx][1]\n",
    "        noisy_audio, sample_rate = torchaudio.load(noisy_audio_path)\n",
    "\n",
    "        noisy_audio = self.normalize_audio(noisy_audio)\n",
    "        noisy_audio = self.adjust_audio_duration(noisy_audio, self.min_duration)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            clean_audio = self.transform(clean_audio, sample_rate)\n",
    "            noisy_audio = self.transform(noisy_audio, sample_rate)\n",
    "\n",
    "        return clean_audio, noisy_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.transform import spec_n_mfcc\n",
    "\n",
    "cleaner_dataset = CoraCleanerDataset(heart_noised, spec_n_mfcc)\n",
    "\n",
    "print(len(cleaner_dataset))\n",
    "cleaner_dataset[5][0].shape, cleaner_dataset[5][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_size = int(train_ratio * len(cleaner_dataset))\n",
    "val_size = int(val_ratio * len(cleaner_dataset))\n",
    "test_size = len(cleaner_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(cleaner_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Métrica a monitorear para determinar el mejor modelo\n",
    "    dirpath='checkpoints/',  # Directorio para guardar los modelos\n",
    "    filename='autoencoder-{epoch:02d}-{val_loss:.4f}',  # Formato del nombre del archivo\n",
    "    save_top_k=1,  # Guardar solo el mejor modelo\n",
    "    mode='min'  # 'min' o 'max' según si la métrica debe ser menor o mayor para ser \"mejor\"\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Métrica a monitorear para detener el entrenamiento\n",
    "    patience=5,  # Número de épocas sin mejora antes de detener\n",
    "    mode='min'  # 'min' o 'max' según si la métrica debe ser menor o mayor para detener\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# callbacks = [checkpoint, early_stopping, lr_monitor]\n",
    "callbacks = [checkpoint, lr_monitor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = Autoencoder(input_size=cleaner_dataset.min_duration, latent_dim=64)\n",
    "\n",
    "trainer = Trainer(max_epochs=3, callbacks=callbacks)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = cleaner_dataset[0][1].reshape(-1)[:2000]\n",
    "noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_audio = model(noisy.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "\n",
    "predicted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clean = cleaner_dataset[0][0].reshape(-1)[:cleaner_dataset.min_duration]\n",
    "noisy = cleaner_dataset[0][1].reshape(-1)[:cleaner_dataset.min_duration]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_audio = model(noisy.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "\n",
    "plot_audio_sample(clean, 'Limpio')\n",
    "plot_audio_sample(noisy, 'Sucio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cora_cleaner-eXlQ0b7N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
